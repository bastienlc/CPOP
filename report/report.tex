\documentclass[11pt]{article}
\usepackage{theme}
\usepackage{shortcuts}
% Document parameters
% Document title
\title{Detecting Changes in Slope With an $L_0$ Penalty}
\author{
Bastien LE CHENADEC \email{bastien.le-chenadec@eleves.enpc.fr} \\ % student 1
Sofiane EZZEHI \email{sofiane.ezzehi@eleves.enpc.fr} % student 2
}

\begin{document}
\maketitle

\paragraph{What is expected for these mini-projects?}
The goal of the exercise is to read (and understand) a research article, implement it (or find an implementation), test it on real data and comment on the results obtained.
Depending on the articles, the task will not always be the same: some articles are more theoretical or complex, others are in the direct line of the course, etc... It is therefore important to balance the exercise according to the article. For example, if you have reused an existing implementation, it is obvious that you will have to develop in a more detailed way the analysis of the results, the influence of the parameters etc... Do not hesitate to contact us by email if you wish to be guided.

\paragraph{The report}
The report must be at most FIVE pages and use this template (excluding references). If needed, additional images and tables can be put in Appendix, but must be discussed in the main document. The report must contain a precise description of the work done, a description of the method, and the results of your tests. Please do not include source code! The report must clearly show the elements that you have done yourself and those that you have reused only, as well as the distribution of tasks within the team (see detailed plan below.)

\paragraph{The source code}
In addition to this report, you will have to send us a Python notebook allowing to launch the code and to test it on data. For the data, you can find it on standard sites like Kaggle, or the site https://timeseriesclassification.com/ which contains a lot of signals!


\paragraph{The oral presentations}
They will last 10 minutes followed by 5 minutes of questions. The plan of the defense is the same as the one of the report: presentation of the work done, description of the method and analysis of the results.


\paragraph{Deadlines}
Two sessions will be available :
\begin{itemize}
    \item \textbf{Session 1}
          \begin{itemize}
              \item Deadline for report: December 18th (23:59)
              \item Oral presentations: December 20th and 22th (precise times TBA)
          \end{itemize}
    \item \textbf{Session 2}
          \begin{itemize}
              \item Deadline for report: January 9th (23:59)
              \item Oral presentations: January, 11th and 12th (precise times TBA)
          \end{itemize}
\end{itemize}

\section{Introduction and contributions}

The Introduction section (indicative length : less than 1 page) should detail the scientific context of the article you chose, as well as the task that you want to solve (especially if you apply it on novel data). \textbf{The last paragraph of the introduction must contain the following information}:
\begin{itemize}
    \item Repartition of work between the two students
    \item Use of available source code or not, percentage of the source code that has been reused, etc.
    \item Use of existing experiments or new experiments (e.g. test of the influence of parameter that was not conducted in the original article, application of the method on a novel task/data set etc.)
    \item Improvement on the original method (e.g. new pre/post processing steps, grid search for optimal parameters etc.)
\end{itemize}

\section{Method}

The Method section (indicative length : 1 to 2 pages) should describe the mathematical aspects of the method in a summarized manner. Only the main steps that are useful for understanding should be highlighted. If relevant, some details on implementation can be provided (but only marginally).

A word on notations : if we have a sequence $x=x_1,\dots,x_n$, we denote $x[s:t]$ the subsequence $x_s,\dots,x_t$.

\subsection{Model}

Let $y=y_1, \dots, y_n$ be successive data points in time. The goal of this method is to find $m$ changepoints $\tau_1,\dots,\tau_m$ such that the data is divided in $m+1$ segments. We let $\tau_0=0$ and $\tau_{m+1}=n$; the segment $j$ is then $y_{\tau_{j-1}+1},\dots,y_{\tau_j}$.

The method considers a piecewise linear model to fit the data. We denote $\phi_{\tau_j}$ the value taken by the model at time $\tau_j$. Under these assumptions, the model writes :
\begin{equation}
    \forall 0\leq i\leq m,\,\forall \tau_i+1\leq t\leq \tau_{i+1},\quad Y_t=\phi_{\tau_i}+\frac{\phi_{\tau_{i+1}}-\phi_{\tau_i}}{\tau_{i+1}-\tau_i}(t-\tau_i)+Z_t
\end{equation}
where $(Z_t)_{1\leq t\leq n}$ is assumed to be a gaussian white noise with variance $\sigma^2$.

\subsection{Penalized cost approach}

The method uses a penalized cost approach to find the changepoints. The cost function is a squarred error loss, penalized by a term that depends on the segments length, and an $L_0$ penalty on the number of changepoints. The cost function naturally writes :
\begin{equation}
    \sum_{i=0}^m \left[\frac{1}{\sigma^2}\sum_{t=\tau_i+1}^{\tau_{i+1}} \left(y_t-\phi_{\tau_i}-\frac{\phi_{\tau_{i+1}}-\phi_{\tau_i}}{\tau_{i+1}-\tau_i}(t-\tau_i)\right)^2 + h(\tau_{i+1}-\tau_i)\right]+\beta m
\end{equation}
By introducing a segment cost, for fitting a segment that takes the value $\phi$ at time $s$ and $\psi$ at time $t$, on the data $y_{s+1},\dots,y_t$, that writes :
\begin{equation}
    \mathcal{C}(y[s+1:t],\phi,\psi) = \frac{1}{\sigma^2}\sum_{j=s+1}^t \left(y_j-\phi-\frac{\psi-\phi}{t-s}(j-s)\right)^2
\end{equation}

the penalized cost function can be rewritten as :
\begin{equation}
    \label{eq:problem}
    \min_{\substack{m\\\tau_1,\dots,\tau_m\\ \phi_0,\dots,\phi_{m+1}}} \beta m+\sum_{i=0}^m \mathcal{C}(y[\tau_i+1:\tau_{i+1}],\phi_{\tau_i},\phi_{\tau_{i+1}}) + h(\tau_{i+1}-\tau_i)
\end{equation}

\subsection{Dynamic programming}

We want to solve the problem \eqref{eq:problem} by dynamic programming. Compared to other changepoints detection methods, the difficulty lies in the continuity constraint between segments. Thus the authors introduce $f^t(\phi)$ as the minimum cost for segmenting $y_1,\dots,y_t$ in $k+1$ segments conditionally on the model taking the value $\phi$ at time $t$ :
\begin{equation}
    \begin{aligned}
        f^t(\phi)=\min_{\substack{\tau_1,\dots,\tau_{k}                         \\ \phi_0,\dots,\phi_k}} &\sum_{i=0}^{k-1} \mathcal{C}(y[\tau_i+1:\tau_{i+1}],\phi_{\tau_i},\phi_{\tau_{i+1}}) + h(\tau_{i+1}-\tau_i)\\
         & +\mathcal{C}(y[\tau_k+1:t],\phi_{\tau_k},\phi)+h(t-\tau_k) + \beta k
    \end{aligned}
\end{equation}
which they are able to express recursively :
\begin{equation}
    f^t(\phi)=\min_{\phi',s} f^s(\phi')+\mathcal{C}(y[s+1:t],\phi',\phi)+h(t-s)+\beta
\end{equation}

They also introduce $f^t_{\boldsymbol{\tau}}(\phi)$ which is the minimum cost for segmenting $y_1,\dots,y_t$ in $k+1$ segments conditionally on the model taking the value $\phi$ at time $t$ and the changepoints being $\boldsymbol{\tau}=(\tau_0,\tau_1,\dots,\tau_k,\tau_{k+1})$. This quantity is a quadratic polynomial in $\phi$ and the authors are able to express its coefficients recursively. All of this allows them to compute $f^n(\phi)$ in $\mathcal{O}(n!/(n-m)!)$.

\subsection{Pruning}

Given the high complexity of the dynamic programming algorithm (exploring the space of all possible changepoints), the authors introduce multiple pruning techniques to reduce the complexity.

\paragraph*{Functional pruning} The authors introduce $\mathcal{T}_t$ the set of all possible changepoint vectors at time $t$, and $\mathcal{T}_t^*=\left\{\boldsymbol{\tau}\in \mathcal{T}_t \:\big|\: \exists\phi\in\mathbb{R}, f^t(\phi)=f_{\boldsymbol{\tau}}^t(\phi)\right\}$ the set of changepoint vectors at time $t$ that are optimal for some $\phi$.

\begin{theorem}
    \label{th:functional_pruning}
    Let $\boldsymbol{\tau}\in\mathcal{T}_s$ such that $\boldsymbol{\tau}\notin \mathcal{T}_s^*$. Then $\boldsymbol{\tau}\notin \mathcal{T}_t^*$ for all $t>s$.
\end{theorem}

This theorem allows us to only explore $\hat{\mathcal{T}}_t=\left\{(\boldsymbol{\tau},s)\:\big|\: 0\leq s\leq t-1, \boldsymbol{\tau}\in \mathcal{T}_s^*\right\}$ when computing $f^t(\phi)$.

\paragraph*{Inequality based pruning} Let $K=2\beta+h(1)+h(n)$. Suppose that $h$ is non-negative and non-decreasing.

\begin{theorem}
    Let $\tau\in\mathcal{T}_s$ such that :
    $$\min_\phi f_{\boldsymbol{\tau}}^s(\phi) > K + \min_{\phi'} f^s(\phi')$$
    Then $\boldsymbol{\tau}\notin \mathcal{T}_t^*$ for all $t>s$.
\end{theorem}

This theorem allows to further prune the set $\hat{\mathcal{T}}_t$.

\subsection{Algorithm}



\section{Data}
The Data section (indicative length : 1 page) should provide a deep analysis of the data used for experiment. In particular, we are interested here in your capacity to provide relevant and thoughtful feedbacks on the data and to demonstrate that you master some "data diagnosis" tools that have been dealt with in the lectures/tutorials.

\section{Results}
The Result section (indicative length : 1 to 2 pages) should display numerical simulations on real data. If you re-used some existing implementations, it is expected that this section develops new experiments that were not present in the original article. Results should be discussed not only based on quantative scores but also on qualitative aspects. In particular (especially if your article focuses on black box methods), please provide some feedbacks whether the method was adapted to the data or not and whether the hypothesis behind the approach you used were validated or not.

\end{document}
